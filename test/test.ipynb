{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "162b18fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 3, 224, 224])\n",
      "VideoMAEConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"VideoMAEForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"decoder_hidden_size\": 384,\n",
      "  \"decoder_intermediate_size\": 1536,\n",
      "  \"decoder_num_attention_heads\": 6,\n",
      "  \"decoder_num_hidden_layers\": 4,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"videomae\",\n",
      "  \"norm_pix_loss\": true,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_frames\": 16,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"tubelet_size\": 2,\n",
      "  \"use_mean_pooling\": false\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, VideoMAEForPreTraining\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "num_frames = 16\n",
    "video = list(np.random.randint(0, 256, (num_frames, 3, 224, 224)))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "model = VideoMAEForPreTraining.from_pretrained(\"MCG-NJU/videomae-base\").to(device)\n",
    "model.eval()\n",
    "\n",
    "pixel_values = image_processor(video, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "num_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\n",
    "seq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\n",
    "bool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\n",
    "\n",
    "# outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\n",
    "# loss = outputs.loss\n",
    "print(pixel_values.shape)\n",
    "print(model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf6a33f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 19 11:42:06 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off |   00000000:19:00.0 Off |                  N/A |\n",
      "| 30%   36C    P2             55W /  350W |     688MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        Off |   00000000:1A:00.0 Off |                  N/A |\n",
      "| 30%   37C    P8             21W /  350W |      19MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090        Off |   00000000:67:00.0 Off |                  N/A |\n",
      "| 30%   35C    P8             11W /  350W |      19MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090        Off |   00000000:68:00.0  On |                  N/A |\n",
      "| 30%   42C    P8             41W /  350W |     649MiB /  24576MiB |     34%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1566      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A     24979      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A     60473      C   ...raudio/anaconda3/envs/er/bin/python        666MiB |\n",
      "|    1   N/A  N/A      1566      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A     24979      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    2   N/A  N/A      1566      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    2   N/A  N/A     24979      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    3   N/A  N/A      1566      G   /usr/lib/xorg/Xorg                             35MiB |\n",
      "|    3   N/A  N/A     24979      G   /usr/lib/xorg/Xorg                            147MiB |\n",
      "|    3   N/A  N/A     25134      G   /usr/bin/gnome-shell                           45MiB |\n",
      "|    3   N/A  N/A     25695      G   ...seed-version=20250515-180047.882000        114MiB |\n",
      "|    3   N/A  N/A     39614      G   ...erProcess --variations-seed-version        274MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2a3d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import librosa\n",
    "import numpy as np\n",
    "from moviepy.editor import VideoFileClip\n",
    "from PIL import Image\n",
    "from scipy.fftpack import dct\n",
    "\n",
    "def save_frame_as_uint8_image(frame, filename):\n",
    "    # Convert RGB frame (float) to uint8 and save\n",
    "    frame_uint8 = np.clip(frame * 255, 0, 255).astype(np.uint8) if frame.dtype == np.float32 or frame.max() <= 1 else frame.astype(np.uint8)\n",
    "    frame_bgr = cv2.cvtColor(frame_uint8, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(filename, frame_bgr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50306d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_audio(audio):\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9df16ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mfcc_as_uint8_image(audio_segment, sr, filename):\n",
    "    mfcc = librosa.feature.mfcc(y=audio_segment, sr=sr, n_mfcc=13)\n",
    "\n",
    "    # Normalize to 0-255 uint8\n",
    "    mfcc_min, mfcc_max = np.min(mfcc), np.max(mfcc)\n",
    "    mfcc_norm = 255 * (mfcc - mfcc_min) / (mfcc_max - mfcc_min + 1e-6)\n",
    "    mfcc_uint8 = mfcc_norm.astype(np.uint8)\n",
    "\n",
    "    # Resize for better visibility if needed (optional)\n",
    "    img = Image.fromarray(mfcc_uint8)\n",
    "    img = img.resize((256, 256), Image.BICUBIC)  # Make all spectrograms same size\n",
    "    img.save(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8dfddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_and_mfccs(video_path, output_dir='output', num_segments=8):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    clip = VideoFileClip(video_path)\n",
    "    duration = clip.duration\n",
    "    audio, sr = librosa.load(video_path, sr=48000)\n",
    "\n",
    "    segment_duration = duration / num_segments\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        time = (i + 0.5) * segment_duration # Time for the middle of the segment\n",
    "        frame = clip.get_frame(time)\n",
    "\n",
    "        # Save video frame as uint8 image\n",
    "        frame_filename = os.path.join(output_dir, f\"frame_{i+1}.jpg\")\n",
    "        save_frame_as_uint8_image(frame, frame_filename)\n",
    "\n",
    "        # Extract audio segment\n",
    "        start_sample = int(i * segment_duration * sr)\n",
    "        end_sample = int((i + 1) * segment_duration * sr)\n",
    "        audio_segment = audio[start_sample:end_sample]\n",
    "\n",
    "        # Save MFCC as uint8 image\n",
    "        # audio_segment = normalize_audio(audio_segment)\n",
    "        # mfcc = MFCC(audio_segment, sr)\n",
    "        # mfcc_min = mfcc.min()\n",
    "        # mfcc_max = mfcc.max()\n",
    "        # mfcc_normalized = 255 * (mfcc - mfcc_min) / (mfcc_max - mfcc_min)\n",
    "        # mfcc_uint8 = mfcc_normalized.astype(np.uint8)\n",
    "\n",
    "        # image = Image.fromarray(mfcc_uint8)\n",
    "        # print(image.shape)\n",
    "\n",
    "        mfcc_filename = os.path.join(output_dir, f\"mfcc_{i+1}.jpg\")\n",
    "        save_mfcc_as_rgb_uint8_image(audio_segment, sr, mfcc_filename, \"viridis\")\n",
    "\n",
    "    print(f\"Saved {num_segments} uint8 frame and MFCC images to '{output_dir}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "536fa9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def save_mfcc_as_rgb_uint8_image(audio, sr, output_path, cmap='viridis'):\n",
    "    # Step 1: Compute MFCC\n",
    "    audio = normalize_audio(audio)\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "\n",
    "    # Step 2: Normalize MFCC to 0-1 for colormap\n",
    "    mfcc_min = mfcc.min()\n",
    "    mfcc_max = mfcc.max()\n",
    "    mfcc_norm = (mfcc - mfcc_min) / (mfcc_max - mfcc_min + 1e-6)  # Prevent div by 0\n",
    "\n",
    "    # Step 3: Map to RGB using a colormap (matplotlib)\n",
    "    colormap = cm.get_cmap(cmap)\n",
    "    mfcc_rgb = colormap(mfcc_norm)[:, :, :3]  # Drop alpha channel if present\n",
    "\n",
    "    # Step 4: Convert to uint8 (0-255)\n",
    "    mfcc_rgb_uint8 = (mfcc_rgb * 255).astype(np.uint8)\n",
    "\n",
    "    # Step 5: Convert to PIL image and save\n",
    "    img = Image.fromarray(mfcc_rgb_uint8)\n",
    "    img.save(output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86cc977c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 8 uint8 frame and MFCC images to 'output'\n"
     ]
    }
   ],
   "source": [
    "extract_frames_and_mfccs(\"/home/varaudio/Thang/emotion_recognition/data/Video_Song_Actor_02/Actor_02/01-02-01-01-01-01-02.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebfd162c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1280, 3)\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('/home/varaudio/Thang/emotion_recognition/test/output/frame_1.jpg')\n",
    "print (image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63da29a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "er",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
